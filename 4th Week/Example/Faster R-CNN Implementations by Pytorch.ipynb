{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Faster R-CNN Implementations by Pytorch"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import cv2\n",
    "from PIL import Image"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "if torch.cuda.is_available():\n",
    "    DEVICE = torch.device(\"cuda\")\n",
    "    print(DEVICE, torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    DEVICE = torch.device(\"cpu\")\n",
    "    print(DEVICE)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Data Preprocessing"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "1) Visualize image and bounding boxes"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "img0 = cv2.imread(\"/home/lee/research/20210627fasterrcnn/drone_city_data/00000067.jpg\")\n",
    "img0 = cv2.cvtColor(img0, cv2.COLOR_BGR2RGB)\n",
    "print(img0.shape)\n",
    "plt.imshow(img0)\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "with open(\"/home/lee/research/20210627fasterrcnn/drone_city_data/00000067.json\", \"r\") as json_file:\n",
    "    data = json.load(json_file)\n",
    "    shapes = data[\"shapes\"]\n",
    "    data_bbox = np.asarray(shapes[0][\"points\"])\n",
    "    print(len(data_bbox))\n",
    "\n",
    "img0_clone = np.copy(img0)\n",
    "print(img0_clone.shape)\n",
    "print(int(data_bbox[0][0]), int(data_bbox[0][1]), int(data_bbox[1][0]), int(data_bbox[1][1]))\n",
    "cv2.rectangle(img0_clone, (int(data_bbox[0][0]), int(data_bbox[0][1])), (int(data_bbox[1][0]), int(data_bbox[1][1])), color=(0,255,0), thickness=10)\n",
    "plt.imshow(img0_clone)\n",
    "plt.show()\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "with open(\"/home/lee/research/20210627fasterrcnn/drone_city_data/00000067.json\", \"r\") as json_file:\n",
    "    data = json.load(json_file)\n",
    "    shapes = data[\"shapes\"]\n",
    "    print(shapes)\n",
    "    data_label = np.asarray(shapes[0][\"label\"])\n",
    "    data_bbox = np.asarray(shapes[0][\"points\"])\n",
    "    print(len(data_bbox), data_label)\n",
    "    "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import json\n",
    "import os\n",
    "CLASSES = ('tree, 나무', 'person, 사람', 'animal, 동물', 'house, 주택', 'apartment, building, 아파트, 빌딩', \n",
    "           'school', 'office, 사무실', 'traffic sign', 'traffic light, 신호등', \n",
    "           'streetlamp, telephone pole', 'banner','milestone, 정상표식', 'bridge, 다리', \n",
    "           'tower, ', 'car_vehicle, 승용차', 'bus_vehicle, 버스', 'truck_vehicle, 트럭', \n",
    "           'motorcycle, bike_vehicle, 오토바이, 자전거')\n",
    "# print(type(CLASSES.index(\"car_vehicle, 승용차\")))\n",
    "\n",
    "\n",
    "with open(\"/home/lee/research/20210627fasterrcnn/drone_city_data/00000067.json\", \"r\") as json_file:\n",
    "    data = json.load(json_file)\n",
    "    shapes = data[\"shapes\"]\n",
    "    bbox_list = []\n",
    "    label_list = []\n",
    "    labels = []\n",
    "    for i in range(len(shapes)):\n",
    "        bbox_list.append(shapes[i][\"points\"])\n",
    "        label_list.append(shapes[i][\"label\"])\n",
    "#     print(\"list points\\n\", bbox)\n",
    "    data_bbox = np.asarray(bbox_list)\n",
    "#     label_array = np.asarray(label_list)\n",
    "#     print(len(label_list))\n",
    "#     print(label_list[0])\n",
    "    for j in range(len(label_list)):\n",
    "        print(label_list[j])\n",
    "        label_index = CLASSES.index(label_list[j])\n",
    "        labels.append(label_index)\n",
    "labels = np.asarray(labels)\n",
    "print(labels)\n",
    "img0_clone = np.copy(img0)\n",
    "# print(img0_clone.shape)\n",
    "for i in range(len(data_bbox)):\n",
    "#     print(int(data_bbox[i][0][0]), int(data_bbox[i][0][1]), int(data_bbox[i][1][0]), int(data_bbox[i][1][1]))\n",
    "    cv2.rectangle(img0_clone, (int(data_bbox[i][0][0]), int(data_bbox[i][0][1])), (int(data_bbox[i][1][0]), int(data_bbox[i][1][1])), color=(0,255,0), thickness=10)\n",
    "plt.imshow(img0_clone)\n",
    "plt.show()\n",
    "\n",
    "    "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "2)Resize image and bounding boxes"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "img = cv2.resize(img0, dsize=(2000, 2000), interpolation=cv2.INTER_CUBIC)\n",
    "plt.figure(figsize=(7, 7))\n",
    "plt.imshow(img)\n",
    "# plt.grid(True, color=\"black\")\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# change the bounding box coordinates\n",
    "# original image size : (3840, 2160)\n",
    "\n",
    "Wratio = 2000/img0.shape[1]\n",
    "Hratio = 2000/img0.shape[0]\n",
    "\n",
    "print(Wratio, Hratio)\n",
    "\n",
    "ratioList = [Wratio, Hratio, Wratio, Hratio]\n",
    "bbox = []\n",
    "\n",
    "for box in data_bbox:\n",
    "    box = [box[0][0], box[0][1], box[1][0], box[1][1]]\n",
    "#     print(box)\n",
    "    box = [int(a*b) for a, b in zip(box, ratioList)]\n",
    "    bbox.append(box)\n",
    "    \n",
    "bbox = np.array(bbox)\n",
    "# print(bbox)\n",
    "\n",
    "img_clone = np.copy(img)\n",
    "for i in range(len(bbox)):\n",
    "#     print(int(data_bbox[i][0][0]), int(data_bbox[i][0][1]), int(data_bbox[i][1][0]), int(data_bbox[i][1][1]))\n",
    "    cv2.rectangle(img_clone, (bbox[i][0], bbox[i][1]), (bbox[i][2], bbox[i][3]), color=(0, 255, 0), thickness=5)\n",
    "plt.imshow(img_clone)\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Define Feature extractor"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "1) Load pretrained VGG16"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "model = torchvision.models.vgg16(pretrained=True).to(DEVICE)\n",
    "features = list(model.features)\n",
    "print(len(features))\n",
    "print(features)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "2) Only collect required layers"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# only collect layers with output feature map size (W, H) < 50\n",
    "\n",
    "dummy_img = torch.zeros((1, 3, 2000, 2000)).float() # test image array\n",
    "print(dummy_img.shape)\n",
    "\n",
    "req_features = []\n",
    "output = dummy_img.clone().to(DEVICE)\n",
    "\n",
    "for feature in features:\n",
    "    output = feature(output)\n",
    "#     print(output.size()) => torch.Size([batch_size, channel, width, height])\n",
    "    if output.size()[2] < 2000//16: # 800/16=50\n",
    "        break\n",
    "    req_features.append(feature)\n",
    "    out_channels = output.size()[1]\n",
    "    \n",
    "print(len(req_features))\n",
    "# print(req_features)\n",
    "print(out_channels)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# convert this list into a Seqeuntial module\n",
    "faster_rcnn_feature_extractor = nn.Sequential(*req_features)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# test the results of the input image pass through the feature extractor\n",
    "\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "imgTensor = transform(img).to(DEVICE)\n",
    "imgTensor = imgTensor.unsqueeze(0)\n",
    "output_map = faster_rcnn_feature_extractor(imgTensor)\n",
    "\n",
    "print(output_map.size())"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# visualize the first 5 channels of the 50*50*512 feature maps\n",
    "\n",
    "imgArray = output_map.data.cpu().numpy().squeeze(0)\n",
    "fig = plt.figure(figsize=(12, 4))\n",
    "figNo = 1\n",
    "\n",
    "for i in range(5):\n",
    "    fig.add_subplot(1, 5, figNo)\n",
    "    plt.imshow(imgArray[i], cmap='gray')\n",
    "    figNo += 1\n",
    "    \n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Generate Anchors Boxes"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "1) Generate Anchors"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# sub-sampling rate = 1/16\n",
    "# image size : 2000x2000\n",
    "# sub-sampled feature map size : 2000 x 1/16 = 125\n",
    "# 50 x 50 = 2500 anchors and each anchor generate 9 anchor boxes\n",
    "# total anchor boxes = 125 x 125 x 9 = 140625\n",
    "# x,y intervals to generate anchor box center\n",
    "\n",
    "feature_size = 2000 // 16\n",
    "ctr_x = np.arange(16, (feature_size + 1) * 16, 16)\n",
    "ctr_y = np.arange(16, (feature_size + 1) * 16, 16)\n",
    "print(len(ctr_x))\n",
    "print(ctr_x)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# coordinates of the 255 center points to generate anchor boxes\n",
    "\n",
    "index = 0\n",
    "ctr = np.zeros((140625, 2))\n",
    "\n",
    "for i in range(len(ctr_x)):\n",
    "    for j in range(len(ctr_y)):\n",
    "        ctr[index, 1] = ctr_x[i] - 8\n",
    "        ctr[index, 0] = ctr_y[j] - 8\n",
    "        index += 1\n",
    "\n",
    "# ctr => [[center x, center y], ...]\n",
    "print(ctr.shape)\n",
    "print(ctr[:50, :])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# display the 2500 anchors within image\n",
    "\n",
    "img_clone2 = np.copy(img)\n",
    "ctr_int = ctr.astype(\"int32\")\n",
    "\n",
    "plt.figure(figsize=(7, 7))\n",
    "for i in range(ctr.shape[0]):\n",
    "    cv2.circle(img_clone2, (ctr_int[i][0], ctr_int[i][1]),\n",
    "              radius=1, color=(255, 0, 0), thickness=3)\n",
    "plt.imshow(img_clone2)\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "2) Generate Anchor boxes"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# for each of the 15,625 anchors, generate 9 anchor boxes\n",
    "# 15,625 x 9 = 140,625 anchor boxes\n",
    "\n",
    "ratios = [0.5, 1, 2]\n",
    "scales = [32, 64, 128]\n",
    "sub_sample = 16\n",
    "print(feature_size)\n",
    "anchor_boxes = np.zeros(((feature_size * feature_size * 9), 4))\n",
    "index = 0\n",
    "\n",
    "for c in ctr:                        # per anchors\n",
    "    ctr_y, ctr_x = c\n",
    "    for i in range(len(ratios)):     # per ratios\n",
    "        for j in range(len(scales)): # per scales\n",
    "            \n",
    "            # anchor box height, width\n",
    "            h = sub_sample * scales[j] * np.sqrt(ratios[i])\n",
    "            w = sub_sample * scales[j] * np.sqrt(1./ ratios[i])\n",
    "#             print(\"index\", index)\n",
    "            # anchor box [x1, y1, x2, y2]\n",
    "            if index < feature_size * feature_size * 9:\n",
    "                anchor_boxes[index, 1] = ctr_y - h / 2.\n",
    "                anchor_boxes[index, 0] = ctr_x - w / 2.\n",
    "                anchor_boxes[index, 3] = ctr_y + h / 2.\n",
    "                anchor_boxes[index, 2] = ctr_x + w / 2.\n",
    "                index += 1\n",
    "            \n",
    "print(anchor_boxes.shape)\n",
    "print(anchor_boxes[:10, :])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# display the anchor boxes of one anchor and the ground truth boxes\n",
    "\n",
    "img_clone = np.copy(img)\n",
    "\n",
    "# draw random anchor boxes\n",
    "for i in range(100000, 100009):\n",
    "    x1 = int(anchor_boxes[i][0])\n",
    "    y1 = int(anchor_boxes[i][1])\n",
    "    x2 = int(anchor_boxes[i][2])\n",
    "    y2 = int(anchor_boxes[i][3])\n",
    "    \n",
    "    cv2.rectangle(img_clone, (x1, y1), (x2, y2), color=(255, 0, 0),\n",
    "                 thickness=3)\n",
    "\n",
    "# draw ground truth boxes\n",
    "for i in range(len(bbox)):\n",
    "    cv2.rectangle(img_clone, (bbox[i][0], bbox[i][1]), \n",
    "                             (bbox[i][2], bbox[i][3]),\n",
    "                 color=(0, 255, 0), thickness=3)\n",
    "\n",
    "plt.imshow(img_clone)\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# draw all anchor boxes\n",
    "\n",
    "# add paddings(can't draw anchor boxes out of image boundary)\n",
    "img_clone3 = np.copy(img)\n",
    "img_clone4 = cv2.copyMakeBorder(img_clone3,400,400,400,400,cv2.BORDER_CONSTANT, value=(255, 255, 255))\n",
    "img_clone5 = np.copy(img_clone4)\n",
    "\n",
    "for i in range(len(anchor_boxes)):\n",
    "    x1 = int(anchor_boxes[i][0])\n",
    "    y1 = int(anchor_boxes[i][1])\n",
    "    x2 = int(anchor_boxes[i][2])\n",
    "    y2 = int(anchor_boxes[i][3])\n",
    "    \n",
    "    cv2.rectangle(img_clone5, (x1+400, y1+400), (x2+400, y2+400), color=(255, 0, 0),\n",
    "                 thickness=3)\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.subplot(121), plt.imshow(img_clone4)\n",
    "plt.subplot(122), plt.imshow(img_clone5)\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Target Anchors"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "1) Only choose anchor boxes inside the image"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# ignore the cross-boundary anchor boxes\n",
    "# valid anchor boxes with (x1, y1) > 0 and (x2, y2) <= 2000\n",
    "\n",
    "index_inside = np.where(\n",
    "        (anchor_boxes[:, 0] >= 0) &\n",
    "        (anchor_boxes[:, 1] >= 0) &\n",
    "        (anchor_boxes[:, 2] <= 2000) &\n",
    "        (anchor_boxes[:, 3] <= 2000))[0]\n",
    "\n",
    "print(index_inside.shape)\n",
    "print(index_inside)\n",
    "# only 100665 anchor boxes are inside the boundary out of 140625\n",
    "valid_anchor_boxes = anchor_boxes[index_inside]\n",
    "print(valid_anchor_boxes.shape)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "2) Calculate IoUs"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# calculate Iou of the valid anchor boxes\n",
    "# since we have 8940 anchor boxes and 4 ground truth objects,\n",
    "# we should get an array with (8940, 4) as the output\n",
    "# [IoU with gt box1, IoU with gt box2, IoU with gt box3,IoU with gt box4]\n",
    "# if number of bboxes are 6, ious = len(valid_anchor_boxes, num_box)\n",
    "num_box = bbox.shape[0]\n",
    "ious = np.empty((len(valid_anchor_boxes),num_box), dtype=np.float32)\n",
    "ious.fill(0)\n",
    "\n",
    "# anchor boxes\n",
    "for i, anchor_box in enumerate(valid_anchor_boxes):\n",
    "    xa1, ya1, xa2, ya2 = anchor_box\n",
    "    anchor_area = (xa2 - xa1) * (ya2 - ya1)\n",
    "    \n",
    "    # ground truth boxes\n",
    "    \n",
    "    for j, gt_box in enumerate(bbox):\n",
    "#         print(\"j\", j, gt_box)\n",
    "        xb1, yb1, xb2, yb2 = gt_box\n",
    "        box_area = (xb2 - xb1) * (yb2 - yb1)\n",
    "#         print(\"box_area\", box_area)\n",
    "        inter_x1 = max([xb1, xa1])\n",
    "        inter_y1 = max([yb1, ya1])\n",
    "        inter_x2 = min([xb2, xa2])\n",
    "        inter_y2 = min([yb2, ya2])\n",
    "\n",
    "        if (inter_x1 < inter_x2) and (inter_y1 < inter_y2):\n",
    "            inter_area = (inter_x2 - inter_x1) * (inter_y2 - inter_y1)\n",
    "            iou = inter_area / (anchor_area + box_area - inter_area)\n",
    "        else:\n",
    "            iou = 0\n",
    "\n",
    "        ious[i, j] = iou\n",
    "\n",
    "print(ious.shape)\n",
    "print(ious[30000:30009:])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "3) Sample positive/negative anchor boxes"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# what anchor box has max ou with the ground truth box\n",
    "\n",
    "gt_argmax_ious = ious.argmax(axis=0)\n",
    "print(gt_argmax_ious)\n",
    "\n",
    "gt_max_ious = ious[gt_argmax_ious, np.arange(ious.shape[1])]\n",
    "print(gt_max_ious)\n",
    "\n",
    "gt_argmax_ious = np.where(ious == gt_max_ious)[0]\n",
    "print(gt_argmax_ious)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# what ground truth bbox is associated with each anchor box\n",
    "\n",
    "argmax_ious = ious.argmax(axis=1)\n",
    "print(argmax_ious.shape)\n",
    "print(argmax_ious)\n",
    "\n",
    "max_ious = ious[np.arange(len(index_inside)), argmax_ious]\n",
    "print(max_ious)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# set the labels of 8940 valid anchor boxes to -1(ignore)\n",
    "\n",
    "label = np.empty((len(index_inside),), dtype=np.int32)\n",
    "label.fill(-1)\n",
    "print(label.shape)\n",
    "# print(label)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# use IoU to assign 1 (objects) to two kind of anchors\n",
    "# a) the anchors with the highest IoU overlap with a ground truth box\n",
    "# b) an anchor that has an IoU overlap higher than 0.7 with ground truth box\n",
    "\n",
    "# Assign 0 (background) to an anchor if its IoU ratio is lower than 0.3\n",
    "\n",
    "pos_iou_threshold = 0.7\n",
    "neg_iou_threshold = 0.3\n",
    "\n",
    "label[gt_argmax_ious] = 1\n",
    "label[max_ious >= pos_iou_threshold] = 1\n",
    "label[max_ious < neg_iou_threshold] = 0\n",
    "print(label)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Every time mini-batch training take only 256 valid anchor boxes to train RPN\n",
    "# of which 128 positive examples, 128 negative-examples\n",
    "# disable leftover positive/negative anchors \n",
    "n_sample = 256\n",
    "pos_ratio = 0.5\n",
    "n_pos = pos_ratio * n_sample\n",
    "\n",
    "pos_index = np.where(label == 1)[0]\n",
    "\n",
    "if len(pos_index) > n_pos:\n",
    "    disable_index = np.random.choice(pos_index,\n",
    "                                    size = (len(pos_index) - n_pos),\n",
    "                                    replace=False)\n",
    "    label[disable_index] = -1\n",
    "    \n",
    "n_neg = n_sample * np.sum(label == 1)\n",
    "neg_index = np.where(label == 0)[0]\n",
    "\n",
    "if len(neg_index) > n_neg:\n",
    "    disable_index = np.random.choice(neg_index, \n",
    "                                    size = (len(neg_index) - n_neg), \n",
    "                                    replace = False)\n",
    "    label[disable_index] = -1"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# convert the format of valid anchor boxes [x1, y1, x2, y2]\n",
    "\n",
    "# For each valid anchor box, find the groundtruth object which has max_iou \n",
    "max_iou_bbox = bbox[argmax_ious]\n",
    "print(max_iou_bbox.shape)\n",
    "\n",
    "height = valid_anchor_boxes[:, 3] - valid_anchor_boxes[:, 1]\n",
    "width = valid_anchor_boxes[:, 2] = valid_anchor_boxes[:, 0]\n",
    "ctr_y = valid_anchor_boxes[:, 1] + 0.5 * height\n",
    "ctr_x = valid_anchor_boxes[:, 0] + 0.5 * width\n",
    "\n",
    "base_height = max_iou_bbox[:, 3] - max_iou_bbox[:, 1]\n",
    "base_width = max_iou_bbox[:, 2] - max_iou_bbox[:, 0]\n",
    "base_ctr_y = max_iou_bbox[:, 1] + 0.5 * base_height\n",
    "base_ctr_x = max_iou_bbox[:, 0] + 0.5 * base_width\n",
    "\n",
    "eps = np.finfo(height.dtype).eps\n",
    "height = np.maximum(height, eps)\n",
    "width = np.maximum(width, eps)\n",
    "\n",
    "dy = (base_ctr_y - ctr_y) / height\n",
    "dx = (base_ctr_x - ctr_x) / width\n",
    "dh = np.log(base_height / height)\n",
    "dw = np.log(base_width / width)\n",
    "\n",
    "anchor_locs = np.vstack((dx, dy, dw, dh)).transpose()\n",
    "print(anchor_locs.shape)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# First set the label=-1 and locations=0 of the 22500 anchor boxes, \n",
    "# and then fill in the locations and labels of the 8940 valid anchor boxes\n",
    "# NOTICE: For each training epoch, we randomly select 128 positive + 128 negative \n",
    "# from 8940 valid anchor boxes, and the others are marked with -1\n",
    "\n",
    "anchor_labels = np.empty((len(anchor_boxes),), dtype=label.dtype)\n",
    "anchor_labels.fill(-1)\n",
    "anchor_labels[index_inside] = label\n",
    "print(anchor_labels.shape)\n",
    "print(anchor_labels[:10])\n",
    "\n",
    "anchor_locations = np.empty((len(anchor_boxes),) + anchor_boxes.shape[1:], dtype=anchor_locs.dtype)\n",
    "anchor_locations.fill(0)\n",
    "anchor_locations[index_inside, :] = anchor_locs\n",
    "print(anchor_locations.shape)\n",
    "print(anchor_locations[:10, :])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# RPN(Region Proposal Network)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "1) Define RPN"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Send the features of the input image to the Region Proposal Network (RPN), \n",
    "# predict 22500 region proposals (ROIs)\n",
    "\n",
    "in_channels = 512\n",
    "mid_channels = 512\n",
    "n_anchor = 9\n",
    "\n",
    "conv1 = nn.Conv2d(in_channels, mid_channels, 3, 1, 1).to(DEVICE)\n",
    "conv1.weight.data.normal_(0, 0.01)\n",
    "conv1.bias.data.zero_()\n",
    "\n",
    "# bounding box regressor\n",
    "reg_layer = nn.Conv2d(mid_channels, n_anchor * 4, 1, 1, 0).to(DEVICE)\n",
    "reg_layer.weight.data.normal_(0, 0.01)\n",
    "reg_layer.bias.data.zero_()\n",
    "\n",
    "# classifier(object or not)\n",
    "cls_layer = nn.Conv2d(mid_channels, n_anchor * 2, 1, 1, 0).to(DEVICE)\n",
    "cls_layer.weight.data.normal_(0, 0.01)\n",
    "cls_layer.bias.data.zero_()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "2) Classification and Bounding box regression"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "x = conv1(output_map.to(DEVICE)) # output_map = faster_rcnn_feature_extractor(imgTensor)\n",
    "pred_anchor_locs = reg_layer(x) # bounding box regresor output\n",
    "pred_cls_scores = cls_layer(x)  # classifier output \n",
    "\n",
    "print(pred_anchor_locs.shape, pred_cls_scores.shape)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Convert RPN to predict the position and classification format of the anchor box\n",
    "# Position: [1, 36(9*4), 50, 50] => [1, 22500(50*50*9), 4] (dy, dx, dh, dw) \n",
    "# Classification: [1, 18(9*2), 50, 50] => [1, 22500, 2] (1, 0)\n",
    "\n",
    "pred_anchor_locs = pred_anchor_locs.permute(0, 2, 3, 1).contiguous().view(1, -1, 4)\n",
    "print(pred_anchor_locs.shape)\n",
    "\n",
    "pred_cls_scores = pred_cls_scores.permute(0, 2, 3, 1).contiguous()\n",
    "print(pred_cls_scores.shape)\n",
    "\n",
    "objectness_score = pred_cls_scores.view(1, 125, 125, 9, 2)[:, :, :, :, 1].contiguous().view(1, -1)\n",
    "print(objectness_score.shape)\n",
    "\n",
    "pred_cls_scores = pred_cls_scores.view(1, -1, 2)\n",
    "print(pred_cls_scores.shape)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# According to the 22500 ROIs predicted by RPN and 22500 anchor boxes, \n",
    "# calculate the RPN loss¶\n",
    "print(pred_anchor_locs.shape)\n",
    "print(pred_cls_scores.shape)\n",
    "print(anchor_locations.shape)\n",
    "print(anchor_labels.shape)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "rpn_loc = pred_anchor_locs[0]\n",
    "rpn_score = pred_cls_scores[0]\n",
    "\n",
    "gt_rpn_loc = torch.from_numpy(anchor_locations)\n",
    "gt_rpn_score = torch.from_numpy(anchor_labels)\n",
    "\n",
    "print(rpn_loc.shape, rpn_score.shape,\n",
    "      gt_rpn_loc.shape, gt_rpn_score.shape)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "3) Multi-task loss"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# For classification we use cross-entropy loss\n",
    "rpn_cls_loss = F.cross_entropy(rpn_score, gt_rpn_score.long().to(DEVICE), ignore_index = -1)\n",
    "print(rpn_cls_loss)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# only positive samples\n",
    "pos = gt_rpn_score > 0\n",
    "mask = pos.unsqueeze(1).expand_as(rpn_loc)\n",
    "print(mask.shape)\n",
    "\n",
    "# take those bounding boxes whick have positive labels\n",
    "mask_loc_preds = rpn_loc[mask].view(-1, 4)\n",
    "mask_loc_targets = gt_rpn_loc[mask].view(-1, 4)\n",
    "print(mask_loc_preds.shape, mask_loc_targets.shape)\n",
    "\n",
    "x = torch.abs(mask_loc_targets.cpu() - mask_loc_preds.cpu())\n",
    "rpn_loc_loss = ((x < 1).float() * 0.5 * x ** 2) + ((x >= 1).float() * (x - 0.5))\n",
    "print(rpn_loc_loss.sum())"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Combining both the rpn_cls_loss and rpn_reg_loss\n",
    "\n",
    "rpn_lambda = 10\n",
    "N_reg = (gt_rpn_score > 0).float().sum()\n",
    "rpn_loc_loss = rpn_loc_loss.sum() / N_reg\n",
    "rpn_loss = rpn_cls_loss + (rpn_lambda * rpn_loc_loss)\n",
    "print(rpn_loss)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Proposal layer"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "1) Transform anchor boxes"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Send the 22500 ROIs predicted by RPN to Fast RCNN to predict bbox + classifications\n",
    "# First use NMS (Non-maximum supression) to reduce 22500 ROI to 2000\n",
    "\n",
    "nms_thresh = 0.7  # non-maximum supression (NMS) \n",
    "n_train_pre_nms = 12000 # no. of train pre-NMS\n",
    "n_train_post_nms = 2000 # after nms, training Fast R-CNN using 2000 RPN proposals\n",
    "n_test_pre_nms = 6000\n",
    "n_test_post_nms = 300 # During testing we evaluate 300 proposals,\n",
    "min_size = 16"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# the labelled 22500 anchor boxes\n",
    "# format converted from [x1, y1, x2, y2] to [ctrx, ctry, w, h]\n",
    "\n",
    "anc_height = anchor_boxes[:, 3] - anchor_boxes[:, 1]\n",
    "anc_width = anchor_boxes[:, 2] - anchor_boxes[:, 0]\n",
    "anc_ctr_y = anchor_boxes[:, 1] + 0.5 * anc_height\n",
    "anc_ctr_x = anchor_boxes[:, 0] + 0.5 * anc_width\n",
    "print(anc_ctr_x.shape)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# The 22500 anchor boxes location and labels predicted by RPN (convert to numpy)\n",
    "# format = (dx, dy, dw, dh)\n",
    "\n",
    "pred_anchor_locs_numpy = pred_anchor_locs[0].cpu().data.numpy()\n",
    "objectness_score_numpy = objectness_score[0].cpu().data.numpy()\n",
    "\n",
    "dy = pred_anchor_locs_numpy[:, 1::4]\n",
    "dx = pred_anchor_locs_numpy[:, 0::4]\n",
    "dh = pred_anchor_locs_numpy[:, 3::4]\n",
    "dw = pred_anchor_locs_numpy[:, 2::4]\n",
    "print(dy.shape)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# ctr_y = dy predicted by RPN * anchor_h + anchor_cy\n",
    "# ctr_x similar\n",
    "# h = exp(dh predicted by RPN) * anchor_h\n",
    "# w similar\n",
    "\n",
    "ctr_y = dy * anc_height[:, np.newaxis] + anc_ctr_y[:, np.newaxis]\n",
    "ctr_x = dx * anc_width[:, np.newaxis] + anc_ctr_x[:, np.newaxis]\n",
    "h = np.exp(dh) * anc_height[:, np.newaxis]\n",
    "w = np.exp(dw) * anc_width[:, np.newaxis]\n",
    "print(w.shape)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "roi = np.zeros(pred_anchor_locs_numpy.shape, dtype=anchor_locs.dtype)\n",
    "roi[:, 0::4] = ctr_x - 0.5 * w\n",
    "roi[:, 1::4] = ctr_y - 0.5 * h\n",
    "roi[:, 2::4] = ctr_x + 0.5 * w\n",
    "roi[:, 3::4] = ctr_y + 0.5 * h\n",
    "\n",
    "print(roi.shape)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "2) Clip the anchor boxes to the image"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# clip the predcited boxes to the image\n",
    "\n",
    "img_size = (2000, 2000)\n",
    "roi[:, slice(0, 4, 2)] = np.clip(roi[:, slice(0, 4, 2)], 0, img_size[0]) # [:, 0, 2]\n",
    "roi[:, slice(1, 4, 2)] = np.clip(roi[:, slice(1, 4, 2)], 0, img_size[1]) # [:, 1, 3]\n",
    "\n",
    "print(roi.shape, np.max(roi), np.min(roi))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# remove predicted boxes with either height or width < threshold\n",
    "\n",
    "hs = roi[:, 3] - roi[:, 1]\n",
    "ws = roi[:, 2] - roi[:, 0]\n",
    "\n",
    "keep = np.where((hs >= min_size) & (ws >= min_size))[0]\n",
    "roi = roi[keep, :]\n",
    "score = objectness_score_numpy[keep]\n",
    "print(keep.shape, roi.shape, score.shape)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "3) Select top-12000 anchor boxes by objectness score"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# sort all (proposal, score) pairs by score from highest to lowest\n",
    "\n",
    "order = score.ravel().argsort()[::-1]\n",
    "print(order.shape)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# take top pre_nms_topN (e.g. 12000 while training and 300 while testing)\n",
    "order = order[:n_train_pre_nms]\n",
    "roi = roi[order, :]\n",
    "print(order.shape, roi.shape)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "4) Non maximum suppression(select 2000 bounding boxes)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# take all the roi boxes\n",
    "x1 = roi[:, 0]\n",
    "y1 = roi[:, 1]\n",
    "x2 = roi[:, 2]\n",
    "y2 = roi[:, 3]\n",
    "\n",
    "# find the areas of all the boxes\n",
    "\n",
    "areas = (x2 - x1 + 1) * (y2 - y1 + 1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# take the indexes of order the probability score in descending order\n",
    "# non maximum suppression\n",
    "\n",
    "order = order.argsort()[::-1]\n",
    "keep = []\n",
    "\n",
    "while (order.size > 0):\n",
    "    i = order[0] # take the 1st elt in roder and append to keep\n",
    "    keep.append(i)\n",
    "\n",
    "    xx1 = np.maximum(x1[i], x1[order[1:]])\n",
    "    yy1 = np.maximum(y1[i], y1[order[1:]])\n",
    "    xx2 = np.minimum(x2[i], x2[order[1:]])\n",
    "    yy2 = np.minimum(y2[i], y2[order[1:]])\n",
    "\n",
    "    w = np.maximum(0.0, xx2 - xx1 + 1)\n",
    "    h = np.maximum(0.0, yy2 - yy1 + 1)\n",
    "\n",
    "    inter = w * h\n",
    "    ovr = inter / (areas[i] + areas[order[1:]] - inter)\n",
    "    inds = np.where(ovr <= nms_thresh)[0]\n",
    "    order = order[inds + 1]\n",
    "\n",
    "keep = keep[:n_train_post_nms] # while training/testing, use accordingly\n",
    "roi = roi[keep]\n",
    "print(len(keep), roi.shape)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Proposal Target layer"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "1) Calculate IoUs"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "n_sample = 128 # number of samples from roi\n",
    "pos_ratio = 0.25 # number of positive examples out of the n_samples\n",
    "pos_iou_thresh = 0.5 # min iou of region proposal with any ground truth object to consider it as positive label\n",
    "neg_iou_thresh_hi = 0.5 # iou 0~0.5 is considered as negative (0, background)\n",
    "neg_iou_thresh_lo = 0.0"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# fine the iou of each ground truth object with the region proposals\n",
    "\n",
    "ious = np.empty((len(roi), bbox.shape[0]), dtype = np.float32)\n",
    "ious.fill(0)\n",
    "\n",
    "for num1, i in enumerate(roi):\n",
    "    ya1, xa1, ya2, xa2 = i\n",
    "    anchor_area = (ya2 - ya1) * (xa2 - xa1)\n",
    "\n",
    "    for num2, j in enumerate(bbox):\n",
    "        yb1, xb1, yb2, xb2 = j\n",
    "        box_area = (yb2 - yb1) * (xb2 - xb1)\n",
    "        inter_x1 = max([xb1, xa1])\n",
    "        inter_y1 = max([yb1, ya1])\n",
    "        inter_x2 = min([xb2, xa2])\n",
    "        inter_y2 = min([yb2, ya2])\n",
    "\n",
    "        if (inter_x1 < inter_x2) and (inter_y1 < inter_y2):\n",
    "            inter_area = (inter_y2 - inter_y1) * (inter_x2 - inter_x1)\n",
    "            iou = inter_area / (anchor_area + box_area - inter_area)\n",
    "        else:\n",
    "            iou = 0\n",
    "        ious[num1, num2] = iou\n",
    "\n",
    "print(ious.shape)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# find out whick ground truth has high IoU for each region proposal\n",
    "# also find the maximum IoU\n",
    "\n",
    "gt_assignment = ious.argmax(axis=1)\n",
    "max_iou = ious.max(axis=1)\n",
    "\n",
    "print(gt_assignment)\n",
    "print(max_iou)\n",
    "\n",
    "# assign the labels to each proposal\n",
    "gt_roi_label = labels[gt_assignment]\n",
    "print(gt_roi_label)"
   ],
   "outputs": [],
   "metadata": {
    "scrolled": true
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "2) Select foreground(positive) samples"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# select the foreground rois as pre the pos_iou_thresh\n",
    "# and n_sample x pos_ratio (128 x 0.25 = 32) foreground samples\n",
    "\n",
    "pos_roi_per_image = 32\n",
    "pos_index = np.where(max_iou >= pos_iou_thresh)[0]\n",
    "pos_roi_per_this_image = int(min(pos_roi_per_image, pos_index.size))\n",
    "\n",
    "if pos_index.size > 0:\n",
    "    pos_index = np.random.choice(\n",
    "    pos_index, size=pos_roi_per_this_image, replace=False)\n",
    "\n",
    "print(pos_roi_per_this_image)\n",
    "print(pos_index)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "3) Select background(negative) samples"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# similarly we do for negative(background) region proposals\n",
    "\n",
    "neg_index = np.where((max_iou < neg_iou_thresh_hi) &\n",
    "                     (max_iou >= neg_iou_thresh_lo))[0]\n",
    "neg_roi_per_this_image = n_sample - pos_roi_per_this_image\n",
    "neg_roi_per_this_image = int(min(neg_roi_per_this_image, neg_index.size))\n",
    "\n",
    "if neg_index.size > 0:\n",
    "    neg_index = np.random.choice(\n",
    "    neg_index, size = neg_roi_per_this_image, replace=False)\n",
    "    \n",
    "print(neg_roi_per_this_image)\n",
    "print(neg_index)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "4) Visualization"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# display RoI samples with positive\n",
    "\n",
    "img_clone = np.copy(img)\n",
    "\n",
    "for i in range(pos_roi_per_this_image):\n",
    "    x1, y1, x2, y2 = roi[pos_index[i]].astype(int)\n",
    "#     print(x1, y1, x2, y2)\n",
    "    cv2.rectangle(img_clone, (x1, y1), (x2, y2), color=(255,255,255),\n",
    "                thickness=3)\n",
    "\n",
    "for i in range(len(bbox)):\n",
    "    cv2.rectangle(img_clone, (bbox[i][0], bbox[i][1]), (bbox[i][2], bbox[i][3]), \n",
    "                color = (255, 0, 0), thickness=3)\n",
    "\n",
    "plt.imshow(img_clone)\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# display RoI samples with negative\n",
    "\n",
    "img_clone = np.copy(img)\n",
    "\n",
    "plt.figure(figsize=(9, 6))\n",
    "\n",
    "for i in range(neg_roi_per_this_image):\n",
    "    x1, y1, x2, y2 = roi[neg_index[i]].astype(int)\n",
    "    cv2.rectangle(img_clone, (x1, y1), (x2, y2), color=(255, 255, 255),\n",
    "                thickness=3)\n",
    "\n",
    "for i in range(len(bbox)):\n",
    "    cv2.rectangle(img_clone, (bbox[i][0], bbox[i][1]), (bbox[i][2], bbox[i][3]), \n",
    "                color = (0, 255, 0), thickness=3)\n",
    "\n",
    "plt.imshow(img_clone)\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "5) Gather positive/negative samples"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# now we gather positive samples index and negative samples index\n",
    "# their respective labels and region proposals\n",
    "\n",
    "keep_index = np.append(pos_index, neg_index)\n",
    "gt_roi_labels = gt_roi_label[keep_index]\n",
    "gt_roi_labels[pos_roi_per_this_image:] = 0 # negative labels => 0\n",
    "sample_roi = roi[keep_index]\n",
    "print(sample_roi.shape)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# pick the ground truth objects for these sample_roi and\n",
    "# later parameterized as we have done while assigning locations to \n",
    "# anchor boxes\n",
    "\n",
    "bbox_for_sampled_roi = bbox[gt_assignment[keep_index]]\n",
    "print(bbox_for_sampled_roi.shape)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "width = sample_roi[:, 2] - sample_roi[:, 0]\n",
    "height = sample_roi[:, 3] - sample_roi[:, 1]\n",
    "ctr_x = sample_roi[:, 0] + 0.5 * width\n",
    "ctr_y = sample_roi[:, 1] + 0.5 * height\n",
    "\n",
    "base_width = bbox_for_sampled_roi[:, 2] - bbox_for_sampled_roi[:, 0]\n",
    "base_height = bbox_for_sampled_roi[:, 3] - bbox_for_sampled_roi[:, 1]\n",
    "base_ctr_x = bbox_for_sampled_roi[:, 0] + 0.5 * base_width\n",
    "base_ctr_y = bbox_for_sampled_roi[:, 1] + 0.5 * base_height"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# transform anchor boxes\n",
    "\n",
    "eps = np.finfo(height.dtype).eps\n",
    "height = np.maximum(height, eps)\n",
    "width = np.maximum(width, eps)\n",
    "\n",
    "dx = (base_ctr_x - ctr_x) / width\n",
    "dy = (base_ctr_y - ctr_y) / height\n",
    "dw = np.log(base_width / width)\n",
    "dh = np.log(base_height / height)\n",
    "\n",
    "gt_roi_locs = np.vstack((dx, dy, dw, dh)).transpose()\n",
    "print(gt_roi_locs.shape)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# RoI pooling"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "1) Concatenate labels with bbox coordinates"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Take out the features of 128 ROI samples and \n",
    "# use max pooling to adjust to the same size, H=7, W=7 (ROI Pooling)\n",
    "\n",
    "rois = torch.from_numpy(sample_roi).float()\n",
    "roi_indices = 0 * np.ones((len(rois),), dtype=np.int32)\n",
    "roi_indices = torch.from_numpy(roi_indices).float()\n",
    "print(rois.shape, roi_indices.shape)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "indices_and_rois = torch.cat([roi_indices[:, None], rois], dim=1)\n",
    "xy_indices_and_rois = indices_and_rois[:, [0, 2, 1, 4, 3]]\n",
    "indices_and_rois = xy_indices_and_rois.contiguous()\n",
    "print(xy_indices_and_rois.shape)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "2) RoI pooling"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "size = (7, 7)\n",
    "adaptive_max_pool = nn.AdaptiveMaxPool2d(size[0], size[1])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "output = []\n",
    "rois = indices_and_rois.data.float()\n",
    "rois[:, 1:].mul_(1/16.0) # sub-sampling ratio\n",
    "rois = rois.long()\n",
    "num_rois = rois.size(0)\n",
    "\n",
    "for i in range(num_rois):\n",
    "    roi = rois[i]\n",
    "    im_idx = roi[0]\n",
    "    im = output_map.narrow(0, im_idx, 1)[..., roi[1]:(roi[3]+1), roi[2]:(roi[4]+1)]\n",
    "    tmp = adaptive_max_pool(im)\n",
    "    output.append(tmp[0])\n",
    "\n",
    "output = torch.cat(output, 0)\n",
    "\n",
    "print(output.size())"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Visualize the first 5 ROI's feature map (for each feature map, only show the 1st channel of d=512)\n",
    "fig=plt.figure(figsize=(12, 4))\n",
    "figNo = 1\n",
    "for i in range(5):\n",
    "    roi = rois[i]\n",
    "    im_idx = roi[0]\n",
    "    im = output_map.narrow(0, im_idx, 1)[..., roi[2]:(roi[4]+1), roi[1]:(roi[3]+1)]\n",
    "    tmp = im[0][0].detach().cpu().numpy()\n",
    "    fig.add_subplot(1, 5, figNo) \n",
    "    plt.imshow(tmp, cmap='gray')\n",
    "    \n",
    "    figNo +=1\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Reshape the tensor so that we can pass it through the feed forward layer.\n",
    "k = output.view(output.size(0), -1)\n",
    "print(k.shape) # 25088 = 7*7*512"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Fast R-CNN"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "1) Classifier and Bounding box regressor"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# 128 boxes + features (7x7x512) of ROI samples are sent to \n",
    "# Detection network to predict the objects bounding box and clas of the input image\n",
    "\n",
    "roi_head_classifier = nn.Sequential(*[nn.Linear(25088, 4096), nn.Linear(4096, 4096)]).to(DEVICE)\n",
    "cls_loc = nn.Linear(4096, 2 * 4).to(DEVICE) # 1 class, 1 background, 4 coordiinates\n",
    "cls_loc.weight.data.normal_(0, 0.01)\n",
    "cls_loc.bias.data.zero_()\n",
    "\n",
    "score = nn.Linear(4096, 2).to(DEVICE) # 1 class, 1 background"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# passing the output of roi pooling to RoI head\n",
    "\n",
    "k = roi_head_classifier(k.to(DEVICE))\n",
    "roi_cls_loc = cls_loc(k)\n",
    "roi_cls_score = score(k)\n",
    "\n",
    "print(roi_cls_loc.shape, roi_cls_score.shape)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Calculate the loss of Fast RCNN based on the gt bboxes and features (h, w, d=512) \n",
    "# corresponding to these 128 ROIs\n",
    "\n",
    "# predicted\n",
    "print(roi_cls_loc.shape)\n",
    "print(roi_cls_score.shape)\n",
    "\n",
    "#actual\n",
    "print(gt_roi_locs.shape)\n",
    "print(gt_roi_labels.shape)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "gt_roi_labels"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "2) Classification loss"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Converting ground truth to torch variable\n",
    "gt_roi_loc = torch.from_numpy(gt_roi_locs)\n",
    "gt_roi_label = torch.from_numpy(np.float32(gt_roi_labels)).long()\n",
    "print(gt_roi_loc.shape, gt_roi_label.shape)\n",
    "\n",
    "#Classification loss\n",
    "roi_cls_loss = F.cross_entropy(roi_cls_score.cpu(), gt_roi_label.cpu(), ignore_index=-1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "3) Regression loss"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# regression loss\n",
    "\n",
    "n_sample = roi_cls_loc.shape[0]\n",
    "roi_loc = roi_cls_loc.view(n_sample, -1, 4)\n",
    "print(roi_loc.shape)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "roi_loc = roi_loc[torch.arange(0, n_sample).long(), gt_roi_label]\n",
    "print(roi_loc.shape)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# for regression we use smooth l1 loss as defined in the Fast R-CNN paper\n",
    "pos = gt_roi_label > 0\n",
    "mask = pos.unsqueeze(1).expand_as(roi_loc)\n",
    "print(mask.shape)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# take those bounding boxes which have positive labels\n",
    "mask_loc_preds = roi_loc[mask].view(-1, 4)\n",
    "mask_loc_targets = gt_roi_loc[mask].view(-1, 4)\n",
    "print(mask_loc_preds.shape, mask_loc_targets.shape)\n",
    "\n",
    "x = torch.abs(mask_loc_targets.cpu() - mask_loc_preds.cpu())\n",
    "roi_loc_loss = ((x < 1).float() * 0.5 * x ** 2) + ((x >= 1).float() * (x - 0.5))\n",
    "print(roi_loc_loss.sum())"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "4) Multi-task loss"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "roi_lambda = 10.\n",
    "roi_loss = roi_cls_loss + (roi_lambda * roi_loc_loss)\n",
    "print(roi_loss)\n",
    "\n",
    "total_loss = rpn_loss + roi_loss\n",
    "print(total_loss)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}